apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: execute-in-docker-dind-cluster
spec:
  inputs:
    params:
      - name: task-pvc
        description: the task pvc - this is the volume where the files (Dockerfile etc..) are expected to be
      - name: ibmcloudApi
        description: the ibmcloud api
        default: https://cloud.ibm.com
      - name: continuous-delivery-context-secret
        description: name of the secret containing the continuous delivery pipeline context secrets
        default: cd-secret
      - name: ibmcloud-apikey-secret-key
        description: field in the secret that contains the api key used to login to ibmcloud
        default: API_KEY
      - name: container-registry-apikey-secret-key
        description: field in the secret that contains the api key used to login to ibmcloud container registry
        default: API_KEY
      - name: resourceGroup
        description: target resource group (name or id) for the ibmcloud login operation
        default: ''
      - name: clusterRegion
        description: (optional) the ibmcloud region hosting the cluster (if none is found it will default to the toolchain region)
        default: ''
      - name: clusterNamespace
        description: (optional) the kubernetes cluster namespace where the docker engine is hosted/deployed
        default: 'build'
      # 
      - name: imageTag
        description: the default image tag if none is provided using the builtImage url
        default: latest
      # Docker Client configuration
      - name: dockerClientImage
        description: The Docker image to use to run the Docker client
        default: docker
      - name: dockerCommands
        description: The docker command(s) to run.
        default: |
          # Default docker build / inspect / push command
          cd /artifacts
          docker build --tag "$IMAGE_URL:$IMAGE_TAG"
          docker inspect ${IMAGE_URL}:${IMAGE_TAG}
          docker push ${IMAGE_URL}:${IMAGE_TAG}
      #
      - name: propertiesFile
        description: file containing properties out of the docker in docker task
        default: build.properties
    resources:
      # Will be set as optional when tekton version support it
      - name: cluster
        type: cluster
  outputs:
    resources:
      - name: builtImage
        type: image
  steps:
    - name: create-registry-secret
      image: ibmcom/pipeline-base-image
      workingDir: /steps
      env:
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: $(inputs.params.continuous-delivery-context-secret)
              key: API_KEY
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e -o pipefail
          # Get the registry url from builtImage
          REGISTRY_URL=$(echo $(outputs.resources.builtImage.url) |  awk -F/ '{print $1}')

          # create a dry-run k8s secret of type docker-registry to obtain
          # the content of a docker config.json file to access the target
          # ibmcloud container registry
          echo "Creating a Kubernetes secret to access the IBM Cloud Container Registry."
          kubectl create secret --dry-run=true --output=json \
            docker-registry registry-dockerconfig-secret \
            --docker-server=${REGISTRY_URL} \
            --docker-password=${API_KEY} \
            --docker-username=iamapikey --docker-email=a@b.com | \
          jq -r '.data[".dockerconfigjson"]' | base64 -d > config.json

          echo ""
          echo "Secret created."
      volumeMounts:
        - mountPath: /steps
          name: steps-volume
    - name: run-docker-commands
      image: $(inputs.params.dockerClientImage)
      env:
        # Docker client configuration
        # Connect to the sidecar over TCP.
        - name: DOCKER_HOST
          value: 'tcp://localhost:2375'
        # The location of the client configuration files.
        - name: DOCKER_CONFIG
          value: /steps
      command: ["/bin/sh", "-c"]
      args:
        - |
          set -e -o pipefail
          IMAGE_URL=$(echo $(outputs.resources.builtImage.url) |  awk -F: '{print $1}')
          IMAGE_TAG=$(echo $(outputs.resources.builtImage.url) |  awk -F: '{print $2}')
          if [ -z "$IMAGE_TAG" ]; then
            IMAGE_TAG="$(inputs.params.imageTag)"
          fi
          echo "Image URL: $IMAGE_URL"
          echo "Image tag: $IMAGE_TAG"
          echo ""

          # run docker command(s) passed as parameter
          echo "Running docker command(s)..."
          echo ""

          $(inputs.params.dockerCommands)

          echo ""
          # Persist registry and image information in an output properties file
          REGISTRY_URL=$(echo $(outputs.resources.builtImage.url) |  awk -F/ '{print $1}')
          REGISTRY_NAMESPACE=$(echo $(outputs.resources.builtImage.url) |  awk -F/ '{print $2}')
          IMAGE_NAME=$(echo $(outputs.resources.builtImage.url) |  awk -F/ '{print $3}')
          MANIFEST_SHA=$(docker images --digests | grep -i $IMAGE_URL | awk '{print $3}')

          if [ "$PROPERTIES_FILE" ]; then
            # Ensure directory is there
            mkdir -p /artifacts/$(dirname "$PROPERTIES_FILE")
            touch /artifacts/$PROPERTIES_FILE
            echo "REGISTRY_URL=${REGISTRY_URL}" >> /artifacts/$PROPERTIES_FILE
            echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}" >> /artifacts/$PROPERTIES_FILE
            echo "IMAGE_NAME=${IMAGE_NAME}" >> /artifacts/$PROPERTIES_FILE
            echo "IMAGE_TAGS=${IMAGE_TAG}" >> /artifacts/$PROPERTIES_FILE
            echo "IMAGE_MANIFEST_SHA=${MANIFEST_SHA}" >> /artifacts/$PROPERTIES_FILE
            echo "$PROPERTIES_FILE content:"
            cat /artifacts/$PROPERTIES_FILE
            echo ""
          fi
          echo "Done."
      volumeMounts:
        - mountPath: /artifacts
          name: task-volume
        - mountPath: /steps
          name: steps-volume
        - mountPath: /cd-config
          name: cd-config-volume
  sidecars:
    - name: setup-docker-build-cluster
      image: ibmcom/pipeline-base-image
      env:
        - name: THE_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP      
        - name: IBM_CLOUD_API_KEY
          valueFrom:
            secretKeyRef:
              name: $(inputs.params.continuous-delivery-context-secret)
              key: $(inputs.params.ibmcloud-apikey-secret-key)
        - name: IBMCLOUD_TARGET_RESOURCE_GROUP
          value: $(inputs.params.resourceGroup)
        - name: IBMCLOUD_TARGET_REGION
          value: $(inputs.params.clusterRegion)
        - name: BUILD_CLUSTER_NAMESPACE
          value: $(inputs.params.clusterNamespace)
      command: ["/bin/bash", "-c"]
      args:
        - |
          set -e -o pipefail
          ##########################################################################
          # Setting HOME explicitly to have ibmcloud plugins available
          # doing the export rather than env definition is a workaround
          # until https://github.com/tektoncd/pipeline/issues/1836 is fixed
          export HOME="/root"
          ##########################################################################

          # if target region is not specified, used the toolchain region
          if [ -z "$IBMCLOUD_TARGET_REGION" ]; then
            IBMCLOUD_TARGET_REGION=$(jq -r '.region_id' /cd-config/toolchain.json | awk -F: '{print $3}')
          fi

          # if target region is in the 'ibm:yp:<region>' format just keep the region part
          REGION_SUBSET=$(echo "$IBMCLOUD_TARGET_REGION" | awk -F ':' '{print $3;}')
          if [ -z "$REGION_SUBSET" ]; then
            echo "IBM Cloud Target Region is $IBMCLOUD_TARGET_REGION"
          else
            export IBMCLOUD_TARGET_REGION=$REGION_SUBSET
            echo "IBM Cloud Target Region is $IBMCLOUD_TARGET_REGION. export IBMCLOUD_TARGET_REGION=$REGION_SUBSET done"
          fi

          echo "Logging in to build cluster account..."
          ibmcloud config --check-version false
          ibmcloud login --apikey "$IBM_CLOUD_API_KEY" -r "$IBMCLOUD_TARGET_REGION"

          if [ -z "$IBMCLOUD_TARGET_RESOURCE_GROUP" ]; then
            echo "Using default resource group" 
          else
            ibmcloud target -g "$IBMCLOUD_TARGET_RESOURCE_GROUP"
          fi

          echo "Cluster list:"
          ibmcloud ks clusters

          echo "Running ibmcloud ks cluster config --cluster "$(inputs.resources.cluster.name)" --export -s"
          CLUSTER_CONFIG_COMMAND=$(ibmcloud ks cluster config --cluster "$(inputs.resources.cluster.name)" --export -s)
          echo "$CLUSTER_CONFIG_COMMAND"
          eval $CLUSTER_CONFIG_COMMAND

          echo "Checking cluster namespace $BUILD_CLUSTER_NAMESPACE"
          if ! kubectl get namespace "$BUILD_CLUSTER_NAMESPACE"; then
            kubectl create namespace "$BUILD_CLUSTER_NAMESPACE"
          fi

          # Ensure there is a Docker server on the build cluster
          if ! kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" rollout status -w deployment/docker-dind; then
            echo "Preparing Docker server deployment"
            cat > /sidecar-only/docker-dind-deployment.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            labels:
              run: docker
            name: docker-dind
          spec:
            replicas: 1
            selector:
              matchLabels:
                run: docker
            template:
              metadata:
                labels:
                  run: docker
              spec:
                containers:
                - name: docker
                  image: docker:dind
                  env:
                  - name: DOCKER_TLS_CERTDIR
                    value: ''
                  securityContext:
                    privileged: true
          EOF
            echo "Installing Docker Server into build cluster..."
            kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" apply -f /sidecar-only/docker-dind-deployment.yaml
            kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" rollout status -w deployment/docker-dind
          fi

          # Use port-forward to make the pod/port locally accessible
          # We need to include the POD IP in the addresses because the readinessProbe will try to open
          # a socket on <POD_IP>:PORT
          # Be sure to use a running POD (not an evicted one)
          kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" get pods           
          kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" port-forward \
            --address=localhost,${THE_POD_IP} \
            $(kubectl --namespace "$BUILD_CLUSTER_NAMESPACE" get pods | grep docker | grep -i running | awk '{print $1;}') \
            2375:2375
      volumeMounts:
        - mountPath: /cd-config
          name: cd-config-volume
        - mountPath: /sidecar-only
          name: sidecar-volume
      # Wait for the port foward to be available
      readinessProbe:
        tcpSocket:
          port: 2375
        initialDelaySeconds: 3
        periodSeconds: 3
        failureThreshold: 10
  volumes:
    - name: task-volume
      persistentVolumeClaim:
        claimName: $(inputs.params.task-pvc)
    - name: sidecar-volume
      emptyDir: {}
    - name: steps-volume
      emptyDir: {}
    - name: cd-config-volume
      configMap:
        name: toolchain
        items:
        - key: toolchain.json
          path: toolchain.json
